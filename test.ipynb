{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer      \n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, config_path='config/model.json'):\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        gpt2_name = config.pop('model_name')\n",
    "        self.template = config.pop('template')\n",
    "\n",
    "        self.device = config.pop('device')\n",
    "        self.config = config\n",
    "        self.context = ''\n",
    "\n",
    "        self.get_model(gpt2_name, config.pop(\"model_cpt\"))\n",
    "\n",
    "    def answer(self, text):\n",
    "        if text[-1] not in {'.', '?', '!'}:\n",
    "            text += '.'\n",
    "\n",
    "        self.context += self.template.format(text)\n",
    "        response = self.generate(self.context)\n",
    "        \n",
    "        answer = self.process_response(response)\n",
    "        self.context += answer\n",
    "\n",
    "        return answer\n",
    "\n",
    "    \n",
    "    def process_response(self, response):\n",
    "        split = re.split('(\\.|\\?|\\!)', response)\n",
    "        if '\\n' in split[0]:\n",
    "            answer = split[0].split('\\n')[0] +  '.'\n",
    "        else:\n",
    "            answer = split[0]\n",
    "            if len(split) > 1:\n",
    "                answer += split[1]\n",
    "        \n",
    "        return answer\n",
    "\n",
    "    def generate(self, text):\n",
    "        max_input_size = self.model.config.n_positions\n",
    "        if 'max_length' in self.config['generate_config']:\n",
    "            max_input_size -= self.config['generate_config']['max_length']\n",
    "        input_ids = self.tokenizer.encode(text, return_tensors='pt').to(self.device)\n",
    "        if len(input_ids) > max_input_size:\n",
    "            input_ids = input_ids[-max_input_size:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = self.model.generate(input_ids, **self.config['generate_config'])\n",
    "        \n",
    "        out = out[:, input_ids.shape[1]:]\n",
    "        generated_text = list(map(self.tokenizer.decode, out))[0]\n",
    "        \n",
    "        return generated_text\n",
    "    \n",
    "    def get_model(self, model_name, cpt_path):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        \n",
    "        if cpt_path is not None:\n",
    "            cpt = torch.load(cpt_path, map_location='cpu')\n",
    "            self.model.load_state_dict(cpt['model_state_dict'])\n",
    "\n",
    "        self.model.to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Никто со мной не говорит.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.answer('Но кто-то же со мной говорит')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Пользователь: Кто именно? Голосовой помощник Кант: Пользователь.Пользователь: А ты кто? Голосовой помощник Кант: Я никто.Пользователь: Но кто-то же со мной говорит. Голосовой помощник Кант: Никто со мной не говорит.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import json\n",
    "import random\n",
    "import telebot\n",
    "from telebot.types import InlineKeyboardButton, InlineKeyboardMarkup\n",
    "from transcribe import ogg2wav, transcribe_audio\n",
    "from parse import parse_message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.config.n_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import torch\n",
    "# from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# class Model:\n",
    "#     def __init__(self, config_path='config/model.json'):\n",
    "#         with open(config_path, 'r') as f:\n",
    "#             config = json.load(f)\n",
    "#         gpt2_name = config.pop('model_name')\n",
    "#         self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_name)\n",
    "#         self.model = GPT2LMHeadModel.from_pretrained(gpt2_name)\n",
    "#         self.model.eval()\n",
    "#         self.device = config.pop('device')\n",
    "#         self.model.to(self.device)\n",
    "#         self.config = config\n",
    "\n",
    "\n",
    "#     def generate(self, text):\n",
    "#         with torch.no_grad():\n",
    "#             # if self.device != 'cpu':\n",
    "#             #     self.model.to(self.device)\n",
    "#             input_ids = self.tokenizer.encode(text, return_tensors='pt').to(self.device)\n",
    "\n",
    "#             out = self.model.generate(input_ids, **self.config)\n",
    "#             generated_text = list(map(self.tokenizer.decode, out))[0]\n",
    "            \n",
    "\n",
    "#             return generated_text\n",
    "\n",
    "from neural import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 10.92 GiB total capacity; 9.41 GiB already allocated; 34.06 MiB free; 9.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m Model()\n",
      "File \u001b[0;32m~/Desktop/projects/tg_notebot/note-bot/neural.py:15\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, config_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[1;32m     14\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     16\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n",
      "File \u001b[0;32m~/Desktop/projects/tg_notebot/env_notebot/lib/python3.9/site-packages/transformers/modeling_utils.py:1682\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1677\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1678\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1679\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1680\u001b[0m     )\n\u001b[1;32m   1681\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1682\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/projects/tg_notebot/env_notebot/lib/python3.9/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    984\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    985\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 987\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/Desktop/projects/tg_notebot/env_notebot/lib/python3.9/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/tg_notebot/env_notebot/lib/python3.9/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 639 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/projects/tg_notebot/env_notebot/lib/python3.9/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/projects/tg_notebot/env_notebot/lib/python3.9/site-packages/torch/nn/modules/module.py:662\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 662\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    663\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    664\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/Desktop/projects/tg_notebot/env_notebot/lib/python3.9/site-packages/torch/nn/modules/module.py:985\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    983\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    984\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 985\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 10.92 GiB total capacity; 9.41 GiB already allocated; 34.06 MiB free; 9.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Привет, как дела? Как дела, а?\\nДа, я вчера в магазине купила себе кофе.\\nТы вроде не жалеешь, что купил коктейль? :)\\nА я вот не могу понять, почему ты не купишь себе вино. Или ты вообще не знаешь про него? И что ты там напишеш, если ты его не пробовала? Мне кажется, ты просто не поняла, о чем я. :(\\nЯ вам скажу, в этом году я не смогла ничего купить. Я просто забыла про кусочек хлеба. А в прошлом году купили мне кашу, и я даже не заметила, когда я ее съела. Но я очень люблю кушать. В прошлый раз я купилась на крем с вишней, который мне очень'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate('Привет, как')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from finance import *\n",
    "\n",
    "import re\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "class SheetWriter:\n",
    "    def __init__(self, cred_path=\"config/gsheets.json\"):\n",
    "        scopes = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']\n",
    "        self.credentials = ServiceAccountCredentials.from_json_keyfile_name(cred_path, scopes) \n",
    "\n",
    "    def write_to_gsheet(self, amount, category, comment):\n",
    "        file = gspread.authorize(self.credentials) \n",
    "        sheet = file.open('финансы').worksheets()[0]\n",
    "        write_row_ind = len(sheet.col_values(1)) + 1\n",
    "        sheet.update(f\"B{write_row_ind}:D{write_row_ind}\", [[amount, category, comment]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_writer = SheetWriter()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skills\n",
    "- save note\n",
    "  - from voice\n",
    "  - add tags\n",
    "  - from multiple messages\n",
    "- add expences\n",
    "  - from voice\n",
    "  - from markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_expense(text):\n",
    "    amount, category, *comment = text.split(' ') \n",
    "    amount = int(re.sub('\\.', '', amount))\n",
    "    comment = ' '.join(comment)\n",
    "    return amount, category, comment\n",
    "\n",
    "class NoteBot(telebot.TeleBot):\n",
    "    def __init__(self, cred_path='config/telegram.json'):\n",
    "        with open(cred_path, 'r') as f:\n",
    "            d = json.load(f)\n",
    "            api_token = d['token']\n",
    "            self.db_path = d['db_path']\n",
    "            self.admin_chat_id = d['chat_id']\n",
    "        super().__init__(api_token)\n",
    "        self.lang = 'ru-RU'\n",
    "        self.tags = []\n",
    "        self.model = Model()\n",
    "        self.wait_value = False\n",
    "        self.text = ''\n",
    "\n",
    "    def transcribe_message(self, message):\n",
    "        self.tags = []\n",
    "        file_info = self.get_file(message.voice.file_id)\n",
    "        voice_file = self.download_file(file_info.file_path)\n",
    "        with open('tmp.ogg', 'wb') as new_file:\n",
    "            new_file.write(voice_file)\n",
    "\n",
    "        wav_path = ogg2wav('tmp.ogg')\n",
    "        transcription = transcribe_audio(wav_path, self.lang)\n",
    "        os.system('rm tmp.*')\n",
    "        return transcription\n",
    "    \n",
    "    def save_last_message(self):\n",
    "        dt = str(datetime.datetime.now())\n",
    "        dt_pfx = re.sub(r'[:]', '-', dt.split('.')[0])\n",
    "        sv_path = os.path.join(self.db_path, f'{dt_pfx}.md')\n",
    "        \n",
    "        note = parse_message(self.last_message, self.tags)\n",
    "        with open(sv_path, 'w') as f:\n",
    "            f.write(note)\n",
    "        self.last_message = ''\n",
    "    \n",
    "    def get_config(self):\n",
    "        return self.model.config\n",
    "    \n",
    "\n",
    "    def handle_expense(self, message):\n",
    "        text = message.text[message.text.index(' ') + 1:]\n",
    "        values = parse_expense(text)\n",
    "        self.expense = values\n",
    "        confirm_message = 'Трата: {}\\nCумма: {}\\nКомментарий: {}'.format(*values)\n",
    "        self.send_message(message.chat.id, confirm_message, reply_markup=expense_markup)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = NoteBot()\n",
    "\n",
    "def expense_markup():\n",
    "    markup = InlineKeyboardMarkup()\n",
    "    markup.row_width = 1\n",
    "    markup.add(InlineKeyboardButton(\"Сохранить\", callback_data='save_expense'))\n",
    "    return markup\n",
    "\n",
    "def voice_markup():\n",
    "    markup = InlineKeyboardMarkup()\n",
    "    markup.row_width = 3\n",
    "    markup.add(InlineKeyboardButton(\"Сохранить заметку\", callback_data='save_note'),\n",
    "               InlineKeyboardButton(\"Сохранить расход\", callback_data='parse_expense')\n",
    "                InlineKeyboardButton(\"Добавить тэг\", callback_data='hashtag'))\n",
    "    return markup\n",
    "\n",
    "\n",
    "@bot.callback_query_handler(func=lambda call: True)\n",
    "def callback_query(call):\n",
    "    if call.data == 'save_note':\n",
    "        if str(bot.chat_id) == str(bot.admin_chat_id):\n",
    "            bot.save_last_message()\n",
    "            bot.answer_callback_query(call.id, \"Note saved\")\n",
    "        else:\n",
    "            bot.send_message(bot.chat_id, \"Ты не Айдар, не буду ничего сохранять!\")\n",
    "            bot.send_message(bot.admin_chat_id, f\"{bot.chat_id} пытается сохранить тебе заметку!\")\n",
    "    elif call.data == 'save_expense':\n",
    "        sheet_writer.write_to_gsheet(*bot.expense)\n",
    "    \n",
    "    elif call.data == 'hashtag':\n",
    "        bot.answer_callback_query(call.id)\n",
    "        bot.wait_value = 'tag'\n",
    "        bot.send_message(bot.chat_id, \"Введи название тега\")\n",
    "    elif call.data == 'continue':\n",
    "        text = bot.model.generate(bot.last_message)\n",
    "        bot.send_message(bot.chat_id, text)\n",
    "\n",
    "\n",
    "@bot.message_handler(commands=['start'])\n",
    "def start_message(message):\n",
    "    bot.last_message = ''\n",
    "    bot.chat_id = message.chat.id\n",
    "    bot.send_message(message.chat.id, 'Привет!')\n",
    "\n",
    "\n",
    "@bot.message_handler(content_types=['voice'])\n",
    "def handle_voice(message):\n",
    "    bot.chat_id = message.chat.id\n",
    "    transcription = bot.transcribe_message(message)\n",
    "    bot.last_message += transcription + ' '\n",
    "    bot.send_message(message.chat.id, transcription, reply_markup=voice_markup())\n",
    "\n",
    "\n",
    "@bot.message_handler(content_types=['text'])\n",
    "def handle_text(message):\n",
    "    bot.chat_id = message.chat.id\n",
    "    if (message.text.startswith('/expense') or ('трата' in message.text.split(' ')[0])):\n",
    "        bot.handle_expense(message)\n",
    "    elif message.text.startswith('/random_number'):\n",
    "        bot.send_message(message.chat.id, random.randint(0, 100))\n",
    "    elif message.text.startswith('/yes_or_no'):\n",
    "        bot.send_message(message.chat.id, random.choice(('yes', 'no')))\n",
    "    elif message.text.startswith('/set_'):\n",
    "        bot.wait_value = message.text.split('/set_')[1]\n",
    "        bot.send_message(message.chat.id, f'set {bot.wait_value} to what value?')\n",
    "    elif message.text.startswith('/config'):\n",
    "        msg = '; '.join([f'{k}-{v}' for k, v in bot.get_config().items()])\n",
    "        bot.send_message(message.chat.id, msg)\n",
    "\n",
    "    elif bot.wait_value == 'tag':\n",
    "        bot.tags.append(message.text)\n",
    "        bot.wait_value = False\n",
    "    elif bot.wait_value:\n",
    "        if '.' in message.text:\n",
    "            bot.model.config[bot.wait_value] = float(message.text)\n",
    "        else:\n",
    "            bot.model.config[bot.wait_value] = int(message.text)\n",
    "        bot.wait_value = False\n",
    "    else:\n",
    "        bot.last_message += message.text + ' '\n",
    "        bot.send_message(message.chat.id, bot.last_message, reply_markup=voice_markup())\n",
    "    \n",
    "\n",
    "bot.infinity_polling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_notebot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a0a77930cfe751691be67eaa6860e6ceb191e3b115e3437e097eeac54955131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
